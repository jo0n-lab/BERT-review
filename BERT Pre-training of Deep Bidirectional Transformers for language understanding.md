# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## Keywords & Terminology

[representation](https://89douner.tistory.com/339?category=1033935); `language model`; [transformer](http://nlp.seas.harvard.edu/annotated-transformer/); `SOTA`; `GLUE`; `encoder`; `decoder`;
`sequence`; `token`; [context vector](https://wikidocs.net/24996); [word embedding](https://wikidocs.net/33520); [WordPiece](http://aidev.co.kr/nlp/7777); `BPE`; `MLM`; `NSP`;

## ì´ˆë¡ & ì„œë¡ 

BERTëŠ” `deep` `bidirectional` `representations` ë°©ì‹ì˜ pre-train + fine tuning model

ê¸°ì¡´ê³¼ëŠ” ë‹¤ë¥´ê²Œ contextì˜ ì¢Œìš°ë¥¼ ì°¸ì¡°í•˜ëŠ” ë°©ì‹ (attend to bidirectional context)

í•œ ê°œì˜ output layer ì¶”ê°€í•˜ì—¬ ì—¬ëŸ¬ê°€ì§€ taskì— ë§ê²Œ í•™ìŠµ (task-specific)

ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ì€ pre-train ëª¨ë¸ì´ ë‹¤ìŒì˜ tasks ì—ì„œ ìƒë‹¹í•œ íš¨ìš©ì„±ì´ ìˆë‹¤ëŠ” ê²ƒì„ ì…ì¦.

`natural language inference (NLI)`;  `paraphrasing`; `named entity recognition`; `question answering (QA)`;


pre-train representation ì´ ì ìš©ë˜ëŠ” downstream ë‘ ê°€ì§€ ë°©ì‹

ë‘ ê°€ì§€ ë°©ì‹ ëª¨ë‘ general language representations í•™ìŠµí•˜ê¸° ìœ„í•´ ê°™ì€ objective functionì˜ ë‹¨ë°©í–¥ ëª¨ë¸ ì ìš© (GPT ì—ì„œ language modeling objective ì ìš©ë¬ì—ˆìŒ)â€”-í•œê³„

- `feature-based`
    
    task-specific architectures ê°€ ì ìš©ë˜ê³  pre-training ë§ˆì €ë„ ì¶”ê°€ì ì¸ feature ì˜ ì¼ë¶€(`ELMo`)
    
- `fine-tuning`
    
    pre-train ì—ì„œ ì–»ì–´ë‚¸ ìµœì†Œí•œì˜ parameters ê°€ fine-tuned (ê¸°íƒ€ íŠœë‹ë˜ë“¯ì´) (`GPT`)
    

ì–‘ë°©í–¥ ëª¨ë¸ì„ ì ìš©í•˜ë©´ ë‹¨ë°©í–¥ ë°©ì‹ì— ë¹„í•´ pre-trained architecture ì— ëŒ€í•´ ë” ë§ì€ ì„ íƒ(ì™¼ìª½-ì˜¤ë¥¸ìª½, ì˜¤ë¥¸ìª½-ì™¼ìª½)ì´ ê°€ëŠ¥í•˜ê³ ,  pre-train ì„ fine-tuning ì ìš© ì‹œ, ê°•ë ¥í•œ íš¨ê³¼ë¥¼ ë‚¸ë‹¤. 

<aside>
ğŸ‘‰ í•´ë‹¹ ë°©ì‹ì€ context(ë¬¸ë§¥)ì„ íŒŒì•…í•˜ëŠ” QA ê°™ì€ token-level tasksì—ì„œ ìœ ë¦¬í•´ì§„ë‹¤.

</aside>

(MLM/NSP) ì ìš©í•œ pre-train model ìœ¼ë¡œ deep bidirectional transformer ì„ í•™ìŠµí•œ ë°©ì‹ BERT ì— ëŒ€í•´ ë‹¤ë£¬ë‹¤. â€¢ `[https://github.com/google-research/bert](https://github.com/google-research/bert)` (ì†ŒìŠ¤ì½”ë“œ ì²¨ë¶€)

## ê´€ë ¨í•˜ì—¬ ì§„í–‰ëœ ì—°êµ¬ (pre-training strategies)

**`Unsupervised Feature-based Approaches`**

 BERTì™€ ê°™ì§„ ì•Šì§€ë§Œ ì°¨ì›ì„ ê¹¨ëŠ”(ë‹¨ë°©í–¥) ì‹œë„ë¡œì„œëŠ” ELMoë¥¼ ë“¤ ìˆ˜ ìˆë‹¤. 

ë¬¸ë§¥ì— ë¯¼ê°í•œ(cotext-sensitive) íŠ¹ì§•ë“¤ì„ [ì™¼ìª½-ì˜¤ë¥¸ìª½, ì˜¤ë¥¸ìª½-ì™¼ìª½]ìœ¼ë¡œë¶€í„° ì¶”ì¶œí•œë‹¤. ì¶”ì¶œí•œ representaionì„ concatanate í•´ì„œ context tokenì„ ì–»ì–´ë‚¸ë‹¤. í•´ë‹¹ ë°©ì‹ì€ major SOTA NLP model benchmarks ì—ì„œ ìš°ì„¸í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤ëŠ” ê²ƒì— ì˜ì˜ê°€ ìˆë‹¤. (íŠ¹íˆ ë¬¸ë§¥ì„ íŒŒì•…í•˜ëŠ” taskì—ì„œ ìš°ì„¸í–ˆìŒ)

í•´ë‹¹ ë°©ì‹ë§ê³ ë„ LSTMì„ í™œìš©í•œ not deep bidirectional ë°©ì‹ë„ ì œì‹œëœ ì ì´ ìˆë‹¤â€¦

**`Unsupervised Fine-tuning Approaches`**
downstream taskì— ì ìš©ë˜ëŠ” parameterì„ ì–»ê¸° ìœ„í•´ pre-training ì ìš©í•œë‹¤. í•´ë‹¹ ë°©ì‹ì˜ ì´ì ì€ pre-training levelì—ì„œ parameterì˜ í•™ìŠµì´ ì§„í–‰ë˜ì—ˆê¸° ë•Œë¬¸ì— downstream levelì—ì„œëŠ” parameterì˜ í•™ìŠµì´ not-expensiveí•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì„ ë‚´ì„¸ì›Œ openAI GPTê°€ SOTAë¥¼ ë”°ëƒˆë‹¤.

## BERT

two steps : *`pre-training` ,* *`fine-tuning`*

### Architecture

![                     ê·¸ë¦¼ 1 : `pre-training` , `fine-tuning` ì˜ architecture ê°€ ê±°ì˜ ë˜‘ê°™ì´ ìƒê¸´ ê²ƒì„ í™•ì¸       ](source/Untitled.png)

                     ê·¸ë¦¼ 1 : `pre-training` , `fine-tuning` ì˜ architecture ê°€ ê±°ì˜ ë˜‘ê°™ì´ ìƒê¸´ ê²ƒì„ í™•ì¸       

               

`pre-training`, `fine-tuning` ì˜ ëª¨ì–‘ì´ ê°™ì€ ê²ƒì„ í™•ì¸ (ì•„ì£¼ ì‘ì€ ì°¨ì´ë§Œ ìˆë‹¤)

`fine-tuning` ì˜ taskì— ë”°ë¼ì„œë„ ëª¨ì–‘ì´ ê°™ì€ ê²ƒì„ í™•ì¸

BERT ëŠ” bidirectional self-attention - transformer encoder (bidirectional)

GPT ëŠ” constrained self-attention - transformer decoder (generative)

### **Input/Output Representations**

BERT ê°€ ì„ì˜ì˜ ë¬¸ì¥ì— ëŒ€ì‘í•  ìˆ˜ ìˆê¸° ìœ„í•´ token sequence ì— í•˜ë‚˜ì˜ ë¬¸ì¥ ë˜ëŠ” ì§ì„ ì´ë£¨ëŠ” ë¬¸ì¥(2ê°œ)ì„ í¬í•¨ì‹œì¼°ë‹¤. (ì—¬ê¸°ì„œ ë¬¸ì¥ì´ë€, ì–¸ì–´ì ìœ¼ë¡œ ëë§ºëŠ” ë¬¸ì¥ì´ ì•„ë‹Œ, ì—°ê²°ëœ textì˜ ë¬´ì‘ìœ„ì ì¸ ì‹œì‘ê³¼ ëìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì—°ê²°ì„ ëœ»í•œë‹¤.)

![                           ê·¸ë¦¼ 2 : sequence ì˜ ì‹œì‘ì„ ì•Œë¦¬ëŠ” CLS í† í°, ë¬¸ì¥ì˜ ëì„ ì•Œë¦¬ëŠ” SEP í† í°](source/Untitled%201.png)

                           ê·¸ë¦¼ 2 : sequence ì˜ ì‹œì‘ì„ ì•Œë¦¬ëŠ” CLS í† í°, ë¬¸ì¥ì˜ ëì„ ì•Œë¦¬ëŠ” SEP í† í°

sequenceì˜ delimeter ë¡œì„œ `CLS` í† í°ì€ sequenceì˜ ì‹œì‘, `SEP` í† í°ì€ ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚¸ë‹¤.

ë˜í•œ ì…ë ¥ í† í°ì— A í˜¹ì€ B ì˜ ì†Œì† ì •ë³´ë¥¼ ì¶”ê°€í•˜ì—¬ ë¬¸ì¥ì„ êµ¬ë¶„í•¨.

### **Pre-training BERT [`MLM` `NSP`]**

`Masked LM` (`MLM`)

í•˜ë‚˜ì˜ sequence ë‚´ì— ë¬´ì‘ìœ„ í† í°ì„ ê³¨ë¼ *`mask`* í™” í•œë‹¤. (*`mask`* í™” : ì›ë˜ í† í°ì„ ë‹¤ë¥¸ textë¡œ ëŒ€ì¹˜)

*`mask`* í† í°ì´ sequence ë‚´ì— ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ì„ ì •í•´ì„œ (15%) *`mask`* ì´ì „ì˜ í† í°ì„ ë§ì¶”ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ

ì´ë ‡ê²Œ í•˜ë¯€ë¡œì¨ *`mask`* í† í°ë§Œ ë§ì¶”ë©´ ë˜ê¸° ë•Œë¬¸ì— input ì„ ìƒˆë¡­ê²Œ ë°”ê¿€ í•„ìš”ê°€ ì—†ìŒ.

ê·¸ëŸ¬ë‚˜ ì´ëŠ” pre-training ì—ë§Œ ì ìš©ë˜ë¯€ë¡œ downstream task ì—ëŠ” *`mask`* í† í°ì— ëŒ€í•œ í•™ìŠµì´ ì „í˜€ ì´ë¤„ì§€ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.

ì´ë¥¼ ì¡°ê¸ˆì´ë¼ë„ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ *`**mask`* í† í° : 80%, ë¬´ì‘ìœ„ í† í° : 10%, ê·¸ëŒ€ë¡œ í† í° :10%** ì˜ í™•ë¥ ì„ ë¶€ì—¬í•œë‹¤. (í™•ë¥ ì ìœ¼ë¡œ ë¶€ì—¬ë˜ëŠ” ê²ƒì´ë¼ ì˜ë¯¸ê°€ ìˆë‹¤.)

ìµœì¢…ì ìœ¼ë¡œ $T_i$ ë¥¼ ê³„ì‚°í•´ì„œ *`mask`* í™” ë˜ê¸° ì´ì „ì˜ í† í°ì„ ì•Œì•„ë§ì¶˜ë‹¤.

`Next Sentence Prediction` (`NSP`)

language modeling objective ì—ì„œ í™•ì¸í•˜ê¸° í˜ë“  ë¬¸ë§¥ íŒŒì•… ìœ„ì£¼ì˜ QA, NLI

ë¬¸ë§¥ íŒŒì•…ì— íš¨ê³¼ì ì¸ ëª¨ë¸ì„ pre-train í•˜ê¸° ìœ„í•´ NSP ì œì‹œ

monolingual corpus(í•˜ë‚˜ì˜ ì–¸ì–´ë¡œë§Œ ì´ë£¨ì–´ì§„ ë§ë­‰ì¹˜)ë¡œ ë§¤ìš° ì‰½ê²Œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ê³ , ì´ì§„í™” ì‹œí‚¬ ìˆ˜ ìˆìŒ(ë§¤ìš° ê°„ë‹¨í•œ ë°©ì‹ì„ì— ë¹„í•´ QA ì™€ NLIì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.)

A, B ë¬¸ì¥ì´ ì…ë ¥ sequence ë¡œ ì£¼ì–´ì§„ë‹¤ë©´, 50%ì˜ í™•ë¥ ë¡œ (ì˜³ì€)Bê°€ ì˜¤ê³  ë‚˜ë¨¸ì§€ì˜ í™•ë¥ ë¡œ ë¬´ì‘ìœ„ ë¬¸ì¥ì´ ì˜¨ë‹¤. ì—¬ê¸°ì„œ $C$ê°€ ë¬¸ì¥ì— ëŒ€í•´ ì˜ˆì¸¡í•œ ê°’ì´ë‹¤.

![                      ê·¸ë¦¼ 1 : `pre-training` , `fine-tuning` ì˜ architecture ê°€ ê±°ì˜ ë˜‘ê°™ì´ ìƒê¸´ ê±¸ í™•ì¸                      ](source/Untitled.png)

                      ê·¸ë¦¼ 1 : `pre-training` , `fine-tuning` ì˜ architecture ê°€ ê±°ì˜ ë˜‘ê°™ì´ ìƒê¸´ ê±¸ í™•ì¸                      

`data source`

BooksCorpus (800M words), English Wikipedia (2,500M words)

**document-level corpus** >> a shuffled sentence-level corpus

### Fine-tuning **BERT**

ì´ˆê°„ë‹¨.

input, outputì„ ê°ˆì•„ë¼ì›Œ ì£¼ê¸°ë§Œ í•˜ë©´ ë‹¤ë¥¸ taskì—ë„ ì ìš© ê°€ëŠ¥.

ì‹¤í—˜ëœ tasks

`paraphrased pair` 

`hypothesis-premise pair` (ì´ë¡ , ê°€ì„¤)

`question answering pair`

`a degenerate text-âˆ… pair in text classification or sequence tagging`

## Experiments & Results Specs

$BERT_{BASE}$ : (L=12, H=768, A=12, Total Param- eters=110M) GPTë‘ ë˜‘ê°™ì€ í¬ê¸°

$BASE_{LARGE}$ : (L=24, H=1024, A=16, Total Parameters=340M)

### GLUE

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-11 á„‹á…©á„’á…® 9.38.29.png](source/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-07-11_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_9.38.29.png)

for BERT_LARGE we found that fine- tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set.

With random restarts, we use the same pre-trained checkpoint but per- form different fine-tuning data shuffling and clas- sifier layer initialization.

Note that BERTBASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking.

We find that BERTLARGE significantly outper- forms BERTBASE across all tasks, especially those with very little training data.

### **SQuAD v1.1, v2.0**

v1.1 ì€ ì§ˆë¬¸ê³¼ ë‹µì´ ì£¼ì–´ì§€ëŠ” datasetì„ ì…ë ¥. answer ì˜ ì‹œì‘ Sí† í°, ëì— E í† í°ì´ ì£¼ì–´ì§„ë‹¤. 

íŠ¹ì´í•œ ì ì€ SEP í† í°ì´ ì£¼ì–´ì§€ì§€ ì•Šì•„ì„œ ë‹µ/ì§ˆë¬¸ì„ ê°€ë¥´ëŠ” delimeterê°€ ì—†ìŒ. ì´ê²ƒì„ ë§ì¶”ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒ.

![Untitled](source/Untitled%202.png)

The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.

In fact, our single BERT model outperforms the top ensemble sys- tem in terms of F1 score.

Without TriviaQA fine-uning data, we only lose 0.1-0.4 F1, still outper- forming all existing systems by a wide margin.

v2.0 ì€ ì§€ë¬¸ì—ì„œ ë‹µì´ ì£¼ì–´ì§€ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆë‹¤. í•´ë‹¹ ê²½ìš°, CLSì— í‘œì‹œí•˜ë„ë¡

![Untitled](source/Untitled%203.png)

## useful links

[https://velog.io/@xuio/NLP-ë…¼ë¬¸ë¦¬ë·°-BERT-Pre-training-of-Deep-Bidirectional-Transformers-forLanguage-Understanding-ìƒí¸](https://velog.io/@xuio/NLP-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-BERT-Pre-training-of-Deep-Bidirectional-Transformers-forLanguage-Understanding-%EC%83%81%ED%8E%B8)